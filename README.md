# Hate-Speech-Detection
Machine Learning programming - FInal Project 
Problem Description:
Hate speech and abusive language detection are crucial tasks in natural language processing (NLP) with a significant impact on society. However, the accuracy of hate speech and abusive language detection models heavily depends on the quality of the datasets they are trained on. It has been observed that many publicly available datasets for hate speech and abusive language detection are biased towards certain racial groups. This can result in models that are more likely to misclassify hate speech or abusive language directed towards certain racial groups, while performing better on other groups.

This problem is particularly concerning as it perpetuates systemic biases and inequalities in society. In addition, it can also have real-world consequences, such as amplifying hate speech and abusive language directed towards certain groups and further marginalizing them.

Context of the Problem:
The above topic is important for several reasons:

Accuracy of hate speech and abusive language detection models: The quality of the datasets used to train hate speech and abusive language detection models heavily influences their accuracy. Biased datasets can result in models that misclassify hate speech or abusive language directed towards certain racial groups, while performing better on other groups. This can have serious real-world consequences, including amplifying hate speech and further marginalizing certain groups.

Addressing systemic biases and inequalities: Bias in hate speech and abusive language detection datasets perpetuates systemic biases and inequalities in society. By identifying and mitigating this bias, we can take steps towards creating fairer and more equitable natural language processing systems.

Ethical considerations: Building unbiased and accurate natural language processing systems is an ethical consideration for researchers and practitioners in the field. By addressing bias in hate speech and abusive language detection datasets, we can work towards building more responsible and ethical NLP systems.

Limitation About other Approaches:
One potential limitation of the approach is that the dataset used for the analysis is not fully representative of the diverse range of hate speech and abusive language that exists online. The dataset was collected from a limited number of sources, such as Twitter and Reddit, and may not capture the full spectrum of offensive language that exists on the internet. This may limit the generalizability of the findings.

Solution:
The random forest model that was created addresses the limitation of the previous approach by providing better accuracy overall. By using a different algorithm that was able to perform well for both classes, this new approach provides a more robust solution for detecting hate speech and abusive language, thereby helping to reduce racial bias in these datasets.
